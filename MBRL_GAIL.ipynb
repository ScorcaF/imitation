{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsTJ1l0Tvq2"
      },
      "source": [
        "# Train an Agent using Generative Adversarial Imitation Learning\n",
        "\n",
        "The idea of generative adversarial imitation learning is to train a discriminator network to distinguish between expert trajectories and learner trajectories.\n",
        "The learner is trained using a traditional reinforcement learning algorithm such as PPO and is rewarded for trajectories that make the discriminator think that it was an expert trajectory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "git clone http://github.com/ScorcaF/imitation\n",
        "cd imitation\n",
        "pip install -e .\n",
        "\n",
        "pip install mbrl\n",
        "pip install omegaconf\n",
        "apt-get install swig\n",
        "pip install matplotlib==3.1.1\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "pip3 install box2d-py\n",
        "pip3 install gym[Box_2D]\n",
        "pip install stable_baselines3\n",
        "\n",
        "\n",
        "git clone https://github.com/ScorcaF/mbrl-lib.git\n",
        "pip install -e \".[dev]\"\n",
        "pip install imitation"
      ],
      "metadata": {
        "id": "ADgLXyCtTyuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBq9kV9vTvq5"
      },
      "source": [
        "As usual, we first need an expert. \n",
        "Note that we now use a variant of the CartPole environment from the seals package, which has fixed episode durations. Read more about why we do this [here](https://imitation.readthedocs.io/en/latest/guide/variable_horizon.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyvirtualdisplay\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "metadata": {
        "id": "O2IuPJ0ycLGW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPWmyuJ1Tvq7"
      },
      "source": [
        "We generate some expert trajectories, that the discriminator needs to distinguish from the learner's trajectories."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from collections import deque\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class CenterDistanceObsWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(2,), dtype=env.observation_space.dtype)\n",
        "\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return self.process_obs(observation)\n",
        "\n",
        "\n",
        "    def process_obs(self, obs):\n",
        "        # Apply following functions\n",
        "        obs = self.green_mask(obs)\n",
        "        obs = self.gray_scale(obs)\n",
        "        obs = self.blur_image(obs)\n",
        "        obs = self.crop(obs)\n",
        "        obs = self.get_distance_from_center(obs)\n",
        "        return obs\n",
        "\n",
        "    def green_mask(self, observation):\n",
        "        hsv = cv2.cvtColor(observation, cv2.COLOR_BGR2HSV)\n",
        "        mask_green = cv2.inRange(hsv, (36, 25, 25), (70, 255, 255))\n",
        "\n",
        "        ## slice the green\n",
        "        imask_green = mask_green > 0\n",
        "        green = np.zeros_like(observation, np.uint8)\n",
        "        green[imask_green] = observation[imask_green]\n",
        "        return (green)\n",
        "\n",
        "    def gray_scale(self, observation):\n",
        "        gray = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "        return gray\n",
        "\n",
        "    def blur_image(self, observation):\n",
        "        blur = cv2.GaussianBlur(observation, (5, 5), 0)\n",
        "        return blur\n",
        "\n",
        "    def canny_edge_detector(self, observation):\n",
        "        canny = cv2.Canny(observation, 50, 150)\n",
        "        return canny\n",
        "\n",
        "    def crop(self, observation):\n",
        "        # cut stats\n",
        "        return observation[65:67, 24:73]\n",
        "\n",
        "    def get_distance_from_center(self, observation):\n",
        "    # find all non zero values in the cropped strip.\n",
        "        # These non zero points(white pixels) corresponds to the edges of the road\n",
        "        nz = cv2.findNonZero(observation)\n",
        "        image_center = 24\n",
        "        if nz is None:\n",
        "            observation = 1.0\n",
        "\n",
        "        center_coord = (nz[:, 0, 0].max() + nz[:, 0, 0].min())/2\n",
        "        #normalize\n",
        "        observation = np.abs((center_coord - image_center))/24\n",
        "        return (observation, observation)\n",
        "\n",
        "#Lines for sanity check\n",
        "env = gym.make(\"CarRacing-v0\")\n",
        "env = CenterDistanceObsWrapper(env)\n",
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kP86htbl2-9",
        "outputId": "074a6e30-fb1f-4abe-ddc7-0f0243e80633"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Track generation: 920..1155 -> 235-tiles track\n",
            "retry to generate track (normal if there are not many instances of this message)\n",
            "Track generation: 1174..1472 -> 298-tiles track\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2708333333333333, 0.2708333333333333)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZDqpUD2Tvq6"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "import gym\n",
        "\n",
        "\n",
        "\n",
        "expert = PPO(\n",
        "    policy=MlpPolicy,\n",
        "    env=env,\n",
        "    seed=0)\n",
        "expert.learn(1000)  # Note: set to 100000 to train a proficient expert"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# def show_video():\n",
        "#   mp4list = glob.glob('video/*.mp4')\n",
        "#   if len(mp4list) > 0:\n",
        "#     mp4 = mp4list[0]\n",
        "#     video = io.open(mp4, 'r+b').read()\n",
        "#     encoded = base64.b64encode(video)\n",
        "#     ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "#                 loop controls style=\"height: 400px;\">\n",
        "#                 <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "#              </video>'''.format(encoded.decode('ascii'))))\n",
        "#   else: \n",
        "#     print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "# def wrap_env(env):\n",
        "#   env = Monitor(env, './video', force=True)\n",
        "#   return env\n",
        "  \n",
        "# env = wrap_env(env)\n",
        "\n",
        "# for trial in range(1):\n",
        "   \n",
        "#     obs = env.reset()\n",
        "#     done = False\n",
        "#     while not done:\n",
        "#         # --- Doing env step using the agent and adding to model dataset ---\n",
        "#       action, _states = expert.predict(obs, deterministic=True)\n",
        "#       obs, rewards, done, info = env.step(action)\n",
        "# env.close()\n",
        "# show_video()"
      ],
      "metadata": {
        "id": "ZI8ALpI3tKWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltuhQP1jTvq7"
      },
      "outputs": [],
      "source": [
        "%cd imitation/src\n",
        "from imitation.data import rollout\n",
        "from imitation.data.wrappers import RolloutInfoWrapper\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "%cd -\n",
        "\n",
        "rollouts = rollout.rollout(\n",
        "    expert,\n",
        "    DummyVecEnv([lambda: RolloutInfoWrapper(CenterDistanceObsWrapper(gym.make(\"CarRacing-v0\")))] * 5),\n",
        "    rollout.make_sample_until(min_timesteps=None, min_episodes=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c88AVqsKTvq8"
      },
      "source": [
        "Now we are ready to set up our GAIL trainer.\n",
        "Note, that the `reward_net` is actually the network of the discriminator.\n",
        "We evaluate the learner before and after training so we can see if it made any progress."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import omegaconf\n",
        "import gym\n",
        "\n",
        "import mbrl.env.reward_fns as reward_fns\n",
        "import mbrl.env.termination_fns as termination_fns\n",
        "import mbrl.models as models\n",
        "import mbrl.planning as planning\n",
        "import mbrl.util.common as common_util\n",
        "import mbrl.util as util\n",
        "\n",
        "\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "mpl.rcParams.update({\"font.size\": 16})\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "seed = 0\n",
        "rng = np.random.default_rng(seed=seed)\n",
        "generator = torch.Generator(device=device)\n",
        "generator.manual_seed(seed)\n",
        "obs_shape = env.observation_space.shape\n",
        "act_shape = env.action_space.shape\n",
        "\n",
        "# NEED to insert DISCRIMINATOR here\n",
        "reward_fn = reward_fns.discriminator\n",
        "# This function allows the model to know if an observation should make the episode end\n",
        "term_fn = termination_fns.no_termination"
      ],
      "metadata": {
        "id": "XVrEvl0M8aMy",
        "outputId": "40b80223-41f5-43fe-b954-12cd4161c1ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trial_length = 200\n",
        "num_trials = 10\n",
        "ensemble_size = 1\n",
        "\n",
        "# Everything with \"???\" indicates an option with a missing value.\n",
        "# Our utility functions will fill in these details using the \n",
        "# environment information\n",
        "cfg_dict = {\n",
        "    # dynamics model configuration\n",
        "    \"dynamics_model\": {\n",
        "        \"model\": \n",
        "        {\n",
        "            \"_target_\": \"mbrl.models.GaussianMLP\",\n",
        "            \"device\": device,\n",
        "            \"num_layers\": 3,\n",
        "            \"ensemble_size\": ensemble_size,\n",
        "            \"hid_size\": 200,\n",
        "            \"in_size\": \"???\",\n",
        "            \"out_size\": \"???\",\n",
        "            \"deterministic\": False,\n",
        "            \"propagation_method\": \"fixed_model\",\n",
        "            # can also configure activation function for GaussianMLP\n",
        "            \"activation_fn_cfg\": {\n",
        "                \"_target_\": \"torch.nn.LeakyReLU\",\n",
        "                \"negative_slope\": 0.01\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    # options for training the dynamics model\n",
        "    \"algorithm\": {\n",
        "        \"learned_rewards\": False,\n",
        "        \"target_is_delta\": True,\n",
        "        \"normalize\": True,\n",
        "    },\n",
        "    # these are experiment specific options\n",
        "    \"overrides\": {\n",
        "        \"trial_length\": trial_length,\n",
        "        \"num_steps\": num_trials * trial_length,\n",
        "        \"model_batch_size\": 32,\n",
        "        \"validation_ratio\": 0.05\n",
        "    }\n",
        "}\n",
        "cfg = omegaconf.OmegaConf.create(cfg_dict)\n",
        "\n",
        "\n",
        "# Create a 1-D dynamics model for this environment\n",
        "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
        "\n",
        "# Create a gym-like environment to encapsulate the model\n",
        "model_env = models.ModelEnv(env, dynamics_model, term_fn, reward_fn, generator=generator)\n",
        "\n",
        "\n",
        "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)"
      ],
      "metadata": {
        "id": "gjEXsSzK9D-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_util.rollout_agent_trajectories(\n",
        "    env,\n",
        "    trial_length, # initial exploration steps\n",
        "    planning.RandomAgent(env),\n",
        "    {}, # keyword arguments to pass to agent.act()\n",
        "    replay_buffer=replay_buffer,\n",
        "    trial_length=trial_length)\n",
        "\n",
        "agent_cfg = omegaconf.OmegaConf.create({\n",
        "    # this class evaluates many trajectories and picks the best one\n",
        "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
        "    \"planning_horizon\": 15,\n",
        "    \"replan_freq\": 1,\n",
        "    \"verbose\": False,\n",
        "    \"action_lb\": \"???\",\n",
        "    \"action_ub\": \"???\",\n",
        "    # this is the optimizer to generate and choose a trajectory\n",
        "    \"optimizer_cfg\": {\n",
        "        \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
        "        \"num_iterations\": 5,\n",
        "        \"elite_ratio\": 0.1,\n",
        "        \"population_size\": 500,\n",
        "        \"alpha\": 0.1,\n",
        "        \"device\": device,\n",
        "        \"lower_bound\": \"???\",\n",
        "        \"upper_bound\": \"???\",\n",
        "        \"return_mean_elites\": True,\n",
        "    }\n",
        "})\n",
        "\n",
        "agent = planning.create_trajectory_optim_agent_for_model(\n",
        "    model_env,\n",
        "    agent_cfg,\n",
        "    num_particles=20\n",
        ")"
      ],
      "metadata": {
        "id": "TjGoSlBZ9IWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2tsoX0XTvq8"
      },
      "outputs": [],
      "source": [
        "%cd imitation/src\n",
        "from imitation.algorithms.adversarial.gail import GAIL\n",
        "from imitation.rewards.reward_nets import BasicRewardNet\n",
        "from imitation.util.networks import RunningNorm\n",
        "%cd -\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "\n",
        "\n",
        "\n",
        "venv = DummyVecEnv([lambda: (CenterDistanceObsWrapper(gym.make(\"CarRacing-v0\"))] * 8)\n",
        "learner = PPO(\n",
        "    env=venv,\n",
        "    policy=MlpPolicy,\n",
        "    batch_size=64,\n",
        "    ent_coef=0.0,\n",
        "    learning_rate=0.0003,\n",
        "    n_epochs=10,\n",
        ")\n",
        "reward_net = BasicRewardNet(\n",
        "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
        ")\n",
        "gail_trainer = GAIL(\n",
        "    demonstrations=rollouts,\n",
        "    demo_batch_size=1024,\n",
        "    gen_replay_buffer_capacity=2048,\n",
        "    n_disc_updates_per_round=4,\n",
        "    venv=venv,\n",
        "    gen_algo=learner,\n",
        "    reward_net=reward_net,\n",
        ")\n",
        "\n",
        "\n",
        "gail_trainer.train(20000)  # Note: set to 300000 for better results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert in train gen\n",
        "\n",
        "train_losses = []\n",
        "val_scores = []\n",
        "\n",
        "def train_callback(_model, _total_calls, _epoch, tr_loss, val_score, _best_val):\n",
        "    train_losses.append(tr_loss)\n",
        "    val_scores.append(val_score.mean().item())   # this returns val score per ensemble model\n",
        "    \n",
        "# Create a trainer for the model\n",
        "model_trainer = models.ModelTrainer(dynamics_model, optim_lr=1e-3, weight_decay=5e-5)\n",
        "num_trials = 10\n",
        "# Create visualization objects\n",
        "fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
        "ax_text = axs[0].text(300, 50, \"\")\n",
        "    \n",
        "# Main PETS loop\n",
        "all_rewards = [0]\n",
        "for trial in range(num_trials):\n",
        "    obs = env.reset()    \n",
        "    agent.reset()\n",
        "    \n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "    steps_trial = 0\n",
        "    update_axes(axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, all_rewards)\n",
        "    while not done:\n",
        "        # --------------- Model Training -----------------\n",
        "        if steps_trial == 0:\n",
        "            dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats --> all the input arrays must have same number of dimensions, but the array at index 0 has 4 dimension(s) and the array at index 1 has 2 dimension(s)\n",
        "            \n",
        "            dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n",
        "                replay_buffer,\n",
        "                batch_size=cfg.overrides.model_batch_size,\n",
        "                val_ratio=cfg.overrides.validation_ratio,\n",
        "                ensemble_size=ensemble_size,\n",
        "                shuffle_each_epoch=True,\n",
        "                bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n",
        "            )\n",
        "                \n",
        "            model_trainer.train(\n",
        "                dataset_train, \n",
        "                dataset_val=dataset_val, \n",
        "                num_epochs=50, \n",
        "                patience=10, \n",
        "                callback=train_callback,\n",
        "            )\n",
        "\n",
        "        # --- Doing env step using the agent and adding to model dataset ---\n",
        "        next_obs, reward, done, _ = common_util.step_env_and_add_to_buffer(\n",
        "            env, obs, agent, {}, replay_buffer)\n",
        "            \n",
        "        update_axes(axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, all_rewards)\n",
        "        \n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "        steps_trial += 1\n",
        "        \n",
        "        if steps_trial == trial_length:\n",
        "            break\n",
        "    \n",
        "    all_rewards.append(total_reward)\n",
        "\n",
        "update_axes(axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, all_rewards, force_update=True)"
      ],
      "metadata": {
        "id": "5tIPXd01Esbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHUXSjSMTvq9"
      },
      "source": [
        "When we look at the histograms of rewards before and after learning, we can see that the learner is not perfect yet, but it made some progress at least.\n",
        "If not, just re-run the above cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D-N0mEjTvq9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(np.mean(learner_rewards_after_training))\n",
        "print(np.mean(learner_rewards_before_training))\n",
        "\n",
        "plt.hist(\n",
        "    [learner_rewards_before_training, learner_rewards_after_training],\n",
        "    label=[\"untrained\", \"trained\"],\n",
        ")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "bd378ce8f53beae712f05342da42c6a7612fc68b19bea03b52c7b1cdc8851b5f"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "3_train_gail.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}