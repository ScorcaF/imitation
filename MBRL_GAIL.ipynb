{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ScorcaF/imitation/blob/master/MBRL_GAIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsTJ1l0Tvq2"
      },
      "source": [
        "# Train an Agent using Generative Adversarial Imitation Learning\n",
        "\n",
        "The idea of generative adversarial imitation learning is to train a discriminator network to distinguish between expert trajectories and learner trajectories.\n",
        "The learner is trained using a traditional reinforcement learning algorithm such as PPO and is rewarded for trajectories that make the discriminator think that it was an expert trajectory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "%%bash\n",
        "\n",
        "git clone http://github.com/ScorcaF/imitation\n",
        "cd imitation\n",
        "pip install -e .\n",
        "\n",
        "pip install mbrl\n",
        "pip install omegaconf\n",
        "apt-get install swig\n",
        "pip install matplotlib==3.1.1\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "pip3 install box2d-py\n",
        "pip3 install gym[Box_2D]\n",
        "pip install stable_baselines3\n",
        "\n",
        "\n",
        "# git clone https://github.com/ScorcaF/mbrl-lib.git\n",
        "# pip install -e \".[dev]\"\n",
        "# pip install imitation"
      ],
      "metadata": {
        "id": "ADgLXyCtTyuY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBq9kV9vTvq5"
      },
      "source": [
        "As usual, we first need an expert. \n",
        "Note that we now use a variant of the CartPole environment from the seals package, which has fixed episode durations. Read more about why we do this [here](https://imitation.readthedocs.io/en/latest/guide/variable_horizon.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyvirtualdisplay\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "metadata": {
        "id": "O2IuPJ0ycLGW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from collections import deque\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class CenterDistanceObsWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(2,), dtype=env.observation_space.dtype)\n",
        "\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return self.process_obs(observation)\n",
        "\n",
        "\n",
        "    def process_obs(self, obs):\n",
        "        # Apply following functions\n",
        "        obs = self.green_mask(obs)\n",
        "        obs = self.gray_scale(obs)\n",
        "        obs = self.blur_image(obs)\n",
        "        obs = self.crop(obs)\n",
        "        obs = self.get_distance_from_center(obs)\n",
        "        return obs\n",
        "\n",
        "    def green_mask(self, observation):\n",
        "        hsv = cv2.cvtColor(observation, cv2.COLOR_BGR2HSV)\n",
        "        mask_green = cv2.inRange(hsv, (36, 25, 25), (70, 255, 255))\n",
        "\n",
        "        ## slice the green\n",
        "        imask_green = mask_green > 0\n",
        "        green = np.zeros_like(observation, np.uint8)\n",
        "        green[imask_green] = observation[imask_green]\n",
        "        return (green)\n",
        "\n",
        "    def gray_scale(self, observation):\n",
        "        gray = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "        return gray\n",
        "\n",
        "    def blur_image(self, observation):\n",
        "        blur = cv2.GaussianBlur(observation, (5, 5), 0)\n",
        "        return blur\n",
        "\n",
        "    def canny_edge_detector(self, observation):\n",
        "        canny = cv2.Canny(observation, 50, 150)\n",
        "        return canny\n",
        "\n",
        "    def crop(self, observation):\n",
        "        # cut stats\n",
        "        return observation[65:67, 24:73]\n",
        "\n",
        "    def get_distance_from_center(self, observation):\n",
        "    # find all non zero values in the cropped strip.\n",
        "        # These non zero points(white pixels) corresponds to the edges of the road\n",
        "        nz = cv2.findNonZero(observation)\n",
        "        image_center = 24\n",
        "        if nz is None:\n",
        "            observation = 1.0\n",
        "\n",
        "        center_coord = (nz[:, 0, 0].max() + nz[:, 0, 0].min())/2\n",
        "        #normalize\n",
        "        observation = np.abs((center_coord - image_center))/24\n",
        "        return (observation, observation)\n",
        "\n",
        "env = gym.make(\"CarRacing-v0\")\n",
        "env = CenterDistanceObsWrapper(env)\n",
        "# env.reset()"
      ],
      "metadata": {
        "id": "1kP86htbl2-9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPWmyuJ1Tvq7"
      },
      "source": [
        "We generate some expert trajectories, that the discriminator needs to distinguish from the learner's trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BZDqpUD2Tvq6"
      },
      "outputs": [],
      "source": [
        "%%capture \n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "import gym\n",
        "\n",
        "\n",
        "expert = PPO(\n",
        "    policy=MlpPolicy,\n",
        "    env=env,\n",
        "    seed=0)\n",
        "expert.learn(1000)  # Note: set to 100000 to train a proficient expert"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# def show_video():\n",
        "#   mp4list = glob.glob('video/*.mp4')\n",
        "#   if len(mp4list) > 0:\n",
        "#     mp4 = mp4list[0]\n",
        "#     video = io.open(mp4, 'r+b').read()\n",
        "#     encoded = base64.b64encode(video)\n",
        "#     ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "#                 loop controls style=\"height: 400px;\">\n",
        "#                 <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "#              </video>'''.format(encoded.decode('ascii'))))\n",
        "#   else: \n",
        "#     print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "# def wrap_env(env):\n",
        "#   env = Monitor(env, './video', force=True)\n",
        "#   return env\n",
        "  \n",
        "# env = wrap_env(env)\n",
        "\n",
        "# for trial in range(1):\n",
        "   \n",
        "#     obs = env.reset()\n",
        "#     done = False\n",
        "#     while not done:\n",
        "#         # --- Doing env step using the agent and adding to model dataset ---\n",
        "#       action, _states = expert.predict(obs, deterministic=True)\n",
        "#       obs, rewards, done, info = env.step(action)\n",
        "# env.close()\n",
        "# show_video()"
      ],
      "metadata": {
        "id": "ZI8ALpI3tKWb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ltuhQP1jTvq7"
      },
      "outputs": [],
      "source": [
        "%%capture \n",
        "%cd imitation/src\n",
        "from imitation.data import rollout\n",
        "from imitation.data.wrappers import RolloutInfoWrapper\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "%cd -\n",
        "\n",
        "rollouts = rollout.rollout(\n",
        "    expert,\n",
        "    DummyVecEnv([lambda: RolloutInfoWrapper(CenterDistanceObsWrapper(gym.make(\"CarRacing-v0\")))] * 5),\n",
        "    rollout.make_sample_until(min_timesteps=None, min_episodes=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import omegaconf\n",
        "import gym\n",
        "\n",
        "import mbrl.env.reward_fns as reward_fns\n",
        "import mbrl.env.termination_fns as termination_fns\n",
        "import mbrl.models as models\n",
        "import mbrl.planning as planning\n",
        "import mbrl.util.common as common_util\n",
        "import mbrl.util as util\n",
        "\n",
        "\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "mpl.rcParams.update({\"font.size\": 16})\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "seed = 0\n",
        "rng = np.random.default_rng(seed=seed)\n",
        "generator = torch.Generator(device=device)\n",
        "generator.manual_seed(seed)\n",
        "obs_shape = env.observation_space.shape\n",
        "act_shape = env.action_space.shape\n",
        "\n",
        "# NEED to insert DISCRIMINATOR here\n",
        "reward_fn = reward_fns.discriminator\n",
        "# This function allows the model to know if an observation should make the episode end\n",
        "term_fn = termination_fns.no_termination"
      ],
      "metadata": {
        "id": "XVrEvl0M8aMy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "trial_length = 200\n",
        "num_trials = 10\n",
        "ensemble_size = 1\n",
        "\n",
        "# Everything with \"???\" indicates an option with a missing value.\n",
        "# Our utility functions will fill in these details using the \n",
        "# environment information\n",
        "cfg_dict = {\n",
        "    # dynamics model configuration\n",
        "    \"dynamics_model\": {\n",
        "        \"model\": \n",
        "        {\n",
        "            \"_target_\": \"mbrl.models.GaussianMLP\",\n",
        "            \"device\": device,\n",
        "            \"num_layers\": 3,\n",
        "            \"ensemble_size\": ensemble_size,\n",
        "            \"hid_size\": 200,\n",
        "            \"in_size\": \"???\",\n",
        "            \"out_size\": \"???\",\n",
        "            \"deterministic\": False,\n",
        "            \"propagation_method\": \"fixed_model\",\n",
        "            # can also configure activation function for GaussianMLP\n",
        "            \"activation_fn_cfg\": {\n",
        "                \"_target_\": \"torch.nn.LeakyReLU\",\n",
        "                \"negative_slope\": 0.01\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    # options for training the dynamics model\n",
        "    \"algorithm\": {\n",
        "        \"learned_rewards\": False,\n",
        "        \"target_is_delta\": True,\n",
        "        \"normalize\": True,\n",
        "    },\n",
        "    # these are experiment specific options\n",
        "    \"overrides\": {\n",
        "        \"trial_length\": trial_length,\n",
        "        \"num_steps\": num_trials * trial_length,\n",
        "        \"model_batch_size\": 32,\n",
        "        \"validation_ratio\": 0.05\n",
        "    }\n",
        "}\n",
        "cfg = omegaconf.OmegaConf.create(cfg_dict)\n",
        "\n",
        "\n",
        "# Create a 1-D dynamics model for this environment\n",
        "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
        "\n",
        "# Create a gym-like environment to encapsulate the model\n",
        "model_env = models.ModelEnv(env, dynamics_model, term_fn, reward_fn, generator=generator)\n",
        "\n",
        "\n",
        "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)"
      ],
      "metadata": {
        "id": "gjEXsSzK9D-K"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "common_util.rollout_agent_trajectories(\n",
        "    env,\n",
        "    trial_length, # initial exploration steps\n",
        "    planning.RandomAgent(env),\n",
        "    {}, # keyword arguments to pass to agent.act()\n",
        "    replay_buffer=replay_buffer,\n",
        "    trial_length=trial_length)\n",
        "\n",
        "agent_cfg = omegaconf.OmegaConf.create({\n",
        "    # this class evaluates many trajectories and picks the best one\n",
        "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
        "    \"planning_horizon\": 15,\n",
        "    \"replan_freq\": 1,\n",
        "    \"verbose\": False,\n",
        "    \"action_lb\": \"???\",\n",
        "    \"action_ub\": \"???\",\n",
        "    # this is the optimizer to generate and choose a trajectory\n",
        "    \"optimizer_cfg\": {\n",
        "        \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
        "        \"num_iterations\": 5,\n",
        "        \"elite_ratio\": 0.1,\n",
        "        \"population_size\": 500,\n",
        "        \"alpha\": 0.1,\n",
        "        \"device\": device,\n",
        "        \"lower_bound\": \"???\",\n",
        "        \"upper_bound\": \"???\",\n",
        "        \"return_mean_elites\": True,\n",
        "    }\n",
        "})\n",
        "\n",
        "agent = planning.create_trajectory_optim_agent_for_model(\n",
        "    model_env,\n",
        "    agent_cfg,\n",
        "    num_particles=20\n",
        ")"
      ],
      "metadata": {
        "id": "TjGoSlBZ9IWB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c88AVqsKTvq8"
      },
      "source": [
        "Now we are ready to set up our GAIL trainer.\n",
        "Note, that the `reward_net` is actually the network of the discriminator.\n",
        "We evaluate the learner before and after training so we can see if it made any progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "u2tsoX0XTvq8"
      },
      "outputs": [],
      "source": [
        "%%capture \n",
        "%cd imitation/src\n",
        "from imitation.algorithms.adversarial.gail import GAIL\n",
        "from imitation.rewards.reward_nets import BasicRewardNet\n",
        "from imitation.util.networks import RunningNorm\n",
        "%cd -\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "\n",
        "\n",
        "\n",
        "venv = DummyVecEnv([lambda: (CenterDistanceObsWrapper(gym.make(\"CarRacing-v0\")))] * 8)\n",
        "\n",
        "reward_net = BasicRewardNet(\n",
        "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
        ")\n",
        "\n",
        "model_trainer = models.ModelTrainer(dynamics_model, optim_lr=1e-3, weight_decay=5e-5)\n",
        "\n",
        "# gail_trainer = GAIL(\n",
        "#     demonstrations=rollouts,\n",
        "#     demo_batch_size=1024,\n",
        "#     gen_replay_buffer_capacity=2048,\n",
        "#     n_disc_updates_per_round=4,\n",
        "#     venv=venv,\n",
        "#     gen_algo=agent,\n",
        "#     reward_net=reward_net,\n",
        "#     cfg=cfg,\n",
        "#     model_trainer=model_trainer,\n",
        "#     dynamics_model=dynamics_model,\n",
        "#     replay_buffer=replay_buffer\n",
        "# )\n",
        "\n",
        "\n",
        "# gail_trainer.train(20000)  # Note: set to 300000 for better results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gail_trainer = GAIL(\n",
        "    demonstrations=rollouts,\n",
        "    demo_batch_size=1024,\n",
        "    gen_replay_buffer_capacity=2048,\n",
        "    n_disc_updates_per_round=4,\n",
        "    venv=venv,\n",
        "    gen_algo=agent,\n",
        "    reward_net=reward_net,\n",
        "    cfg=cfg,\n",
        "    model_trainer=model_trainer,\n",
        "    dynamics_model=dynamics_model,\n",
        "    replay_buffer=replay_buffer,\n",
        "    gen_train_timesteps=num_trials * trial_length\n",
        ")\n",
        "\n",
        "\n",
        "gail_trainer.train(20000)  # Note: set to 300000 for better results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "100qspSCOk22",
        "outputId": "f61af4f6-cff5-4a56-e419-933591aea736"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Track generation: 919..1153 -> 234-tiles track\n",
            "Track generation: 1019..1284 -> 265-tiles track\n",
            "Track generation: 1077..1350 -> 273-tiles track\n",
            "Track generation: 1179..1486 -> 307-tiles track\n",
            "Track generation: 1187..1488 -> 301-tiles track\n",
            "Track generation: 1282..1605 -> 323-tiles track\n",
            "Track generation: 1020..1282 -> 262-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1159..1453 -> 294-tiles track\n",
            "Track generation: 1156..1449 -> 293-tiles track\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rround:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Track generation: 1063..1333 -> 270-tiles track\n",
            "Track generation: 1184..1484 -> 300-tiles track\n",
            "Track generation: 1251..1576 -> 325-tiles track\n",
            "Track generation: 1254..1575 -> 321-tiles track\n",
            "retry to generate track (normal if there are not manyinstances of this message)\n",
            "Track generation: 1098..1377 -> 279-tiles track\n",
            "Track generation: 1343..1683 -> 340-tiles track\n",
            "Track generation: 1096..1374 -> 278-tiles track\n",
            "Track generation: 1249..1566 -> 317-tiles track\n",
            "Track generation: 974..1228 -> 254-tiles track\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rround:   0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-9b94d7484e20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mgail_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Note: set to 300000 for better results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/imitation/src/imitation/algorithms/adversarial/common.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, total_timesteps, callback)\u001b[0m\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"round\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_train_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_disc_updates_per_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/imitation/src/imitation/algorithms/adversarial/common.py\u001b[0m in \u001b[0;36mtrain_gen\u001b[0;34m(self, total_timesteps, learn_kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;31m# --- Doing env step using the agent and adding to model dataset ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                     next_obs, reward, done, _ = common_util.step_env_and_add_to_buffer(\n\u001b[0;32m--> 455\u001b[0;31m                         self.venv_train, obs, self.gen_algo, {}, self.replay_buffer)\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mbrl/util/common.py\u001b[0m in \u001b[0;36mstep_env_and_add_to_buffer\u001b[0;34m(env, obs, agent, agent_kwargs, replay_buffer, callback)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \"\"\"\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0magent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m     \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mbrl/planning/trajectory_opt.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, optimizer_callback, **_kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             plan = self.optimizer.optimize(\n\u001b[0;32m--> 650\u001b[0;31m                 \u001b[0mtrajectory_eval_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             )\n\u001b[1;32m    652\u001b[0m             \u001b[0mplan_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mbrl/planning/trajectory_opt.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, trajectory_eval_fn, callback)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mtrajectory_eval_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_solution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         )\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_last_solution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mbrl/planning/trajectory_opt.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, obj_fun, x0, callback, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mpopulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstrained_var\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mbrl/planning/trajectory_opt.py\u001b[0m in \u001b[0;36mtrajectory_eval_fn\u001b[0;34m(action_sequences)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mtrajectory_eval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrajectory_eval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mbrl/planning/trajectory_opt.py\u001b[0m in \u001b[0;36mtrajectory_eval_fn\u001b[0;34m(initial_state, action_sequences)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrajectory_eval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         return model_env.evaluate_action_sequences(\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0maction_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_particles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m         )\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mbrl/models/model_env.py\u001b[0m in \u001b[0;36mevaluate_action_sequences\u001b[0;34m(self, action_sequences, initial_state, num_particles)\u001b[0m\n\u001b[1;32m    178\u001b[0m             )\n\u001b[1;32m    179\u001b[0m             _, rewards, dones, model_state = self.step(\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             )\n\u001b[1;32m    182\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterminated\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mbrl/models/model_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions, model_state, sample)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mmodel_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mrng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             )\n\u001b[1;32m    122\u001b[0m             rewards = (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mbrl/models/one_dim_tr_model.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, act, model_state, deterministic, rng)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[1;32m    281\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"obs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mmodel_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"obs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sample_1d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mbrl/models/one_dim_tr_model.py\u001b[0m in \u001b[0;36m_get_model_input\u001b[0;34m(self, obs, action)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mmodel_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_normalizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mmodel_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_normalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 80000 but got size 10000 for tensor number 1 in the list."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHUXSjSMTvq9"
      },
      "source": [
        "When we look at the histograms of rewards before and after learning, we can see that the learner is not perfect yet, but it made some progress at least.\n",
        "If not, just re-run the above cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D-N0mEjTvq9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(np.mean(learner_rewards_after_training))\n",
        "print(np.mean(learner_rewards_before_training))\n",
        "\n",
        "plt.hist(\n",
        "    [learner_rewards_before_training, learner_rewards_after_training],\n",
        "    label=[\"untrained\", \"trained\"],\n",
        ")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "bd378ce8f53beae712f05342da42c6a7612fc68b19bea03b52c7b1cdc8851b5f"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "3_train_gail.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}